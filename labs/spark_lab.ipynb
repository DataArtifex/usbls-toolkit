{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03c53759-7b3c-4729-b8dd-5c390e302704",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/12 16:48:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector(spark://macplgah.cg.shawcable.net:63367/jars/clickhouse-jdbc-0.6.0.jar)\n"
     ]
    }
   ],
   "source": [
    "# https://clickhouse.com/docs/en/integrations/python\n",
    "# https://clickhouse.com/docs/en/integrations/java\n",
    "# https://clickhouse.com/docs/en/sql-reference/data-types\n",
    "# https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html\n",
    "import clickhouse_connect\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import yaml\n",
    "spark = SparkSession.builder\\\n",
    "    .appName(\"CSV to PostgreSQL\") \\\n",
    "    .config(\"spark.jars\", \"clickhouse-jdbc-0.6.0.jar\")\\\n",
    "    .getOrCreate()\n",
    "print(spark.sparkContext._jsc.sc().listJars())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d230b274-d8df-41b1-b4ee-673716fe4379",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read the CSV file\n",
    "data_file = \"TimeSeries/cu/cu.data.1.Allitems\"\n",
    "data_file = \"TimeSeries/cu/cu.data.0.Current\"\n",
    "df = spark.read.option(\"delimiter\", \"\\t\").csv(data_file, header=True, inferSchema=True)\n",
    "\n",
    "# remove trailing spaces in column names\n",
    "for column in df.columns:\n",
    "    df = df.withColumnRenamed(column, column.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "766cddef-e15e-4474-a4d5-cfa344603c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- series_id: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- period: string (nullable = true)\n",
      " |-- value: double (nullable = true)\n",
      " |-- footnote_codes: string (nullable = true)\n",
      "\n",
      "+-----------------+----+------+-----+--------------+\n",
      "|        series_id|year|period|value|footnote_codes|\n",
      "+-----------------+----+------+-----+--------------+\n",
      "|CUSR0000SA0      |1997|   M01|159.4|          NULL|\n",
      "|CUSR0000SA0      |1997|   M02|159.7|          NULL|\n",
      "|CUSR0000SA0      |1997|   M03|159.8|          NULL|\n",
      "|CUSR0000SA0      |1997|   M04|159.9|          NULL|\n",
      "|CUSR0000SA0      |1997|   M05|159.9|          NULL|\n",
      "|CUSR0000SA0      |1997|   M06|160.2|          NULL|\n",
      "|CUSR0000SA0      |1997|   M07|160.4|          NULL|\n",
      "|CUSR0000SA0      |1997|   M08|160.8|          NULL|\n",
      "|CUSR0000SA0      |1997|   M09|161.2|          NULL|\n",
      "|CUSR0000SA0      |1997|   M10|161.5|          NULL|\n",
      "|CUSR0000SA0      |1997|   M11|161.7|          NULL|\n",
      "|CUSR0000SA0      |1997|   M12|161.8|          NULL|\n",
      "|CUSR0000SA0      |1998|   M01|162.0|          NULL|\n",
      "|CUSR0000SA0      |1998|   M02|162.0|          NULL|\n",
      "|CUSR0000SA0      |1998|   M03|162.0|          NULL|\n",
      "|CUSR0000SA0      |1998|   M04|162.2|          NULL|\n",
      "|CUSR0000SA0      |1998|   M05|162.6|          NULL|\n",
      "|CUSR0000SA0      |1998|   M06|162.8|          NULL|\n",
      "|CUSR0000SA0      |1998|   M07|163.2|          NULL|\n",
      "|CUSR0000SA0      |1998|   M08|163.4|          NULL|\n",
      "+-----------------+----+------+-----+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "765a5ecd-95c7-4206-86f5-5dec9a0c522e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE TABLE IF NOT EXISTS bls.tbl_cu_1(series_id String,year Int32,period String,value Float64,footnote_codes  Nullable(String)) ENGINE = MergeTree() PRIMARY KEY (series_id, year, period)\n"
     ]
    }
   ],
   "source": [
    "# generate SQL to create table using clickhouse connect \n",
    "mapping = {\n",
    "    'string': 'String',\n",
    "    'binary': 'String',  # ClickHouse stores binary as String\n",
    "    'boolean': 'UInt8',  # Boolean in ClickHouse is commonly represented as UInt8\n",
    "    'date': 'Date',\n",
    "    'timestamp': 'DateTime',\n",
    "    'double': 'Float64',\n",
    "    'float': 'Float32',\n",
    "    'byte': 'Int8',\n",
    "    'short': 'Int16',\n",
    "    'int': 'Int32',\n",
    "    'long': 'Int64',\n",
    "    'decimal': 'Decimal',  # May need precision and scale: Decimal(P, S)\n",
    "}\n",
    "# generate table column types\n",
    "table_column_types = []\n",
    "for field in df.schema.fields:\n",
    "    null_count = df.filter(col(field.name).isNull()).count()\n",
    "    name = field.name\n",
    "    type = mapping[field.dataType.simpleString()]\n",
    "    if null_count:\n",
    "        type = f\" Nullable({type})\"\n",
    "    column_definition = f\"{name} {type}\"\n",
    "    table_column_types.append(column_definition)\n",
    "\n",
    "table_name = 'bls.tbl_cu_1'\n",
    "sql = f\"CREATE TABLE IF NOT EXISTS {table_name}({','.join(table_column_types)}) ENGINE = MergeTree() PRIMARY KEY (series_id, year, period)\"\n",
    "print(sql)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "1ed78467-7bae-4073-9b81-48f6ef34fad2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<clickhouse_connect.driver.summary.QuerySummary at 0x7f8f38cd7760>"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = clickhouse_connect.get_client(host='localhost', port=8123, username='default', password='', database='bls')\n",
    "client.command(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3f6a93-15df-41aa-bbe2-255404789988",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ETL using Spark\n",
    "url = f\"jdbc:clickhouse://localhost:8123/bls\"\n",
    "properties = {\n",
    "    \"user\": 'default',\n",
    "    \"password\": '',\n",
    "    \"driver\": \"com.clickhouse.jdbc.ClickHouseDriver\",\n",
    "}\n",
    "# Write the DataFrame to PostgreSQL\n",
    "df.write.jdbc(url=url, table=table_name, mode='append', properties=properties)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a5b935-8e8d-4d2e-807b-288f7669a098",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
